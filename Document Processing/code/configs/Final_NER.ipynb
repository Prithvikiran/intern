{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772624bb-dd53-494b-80d1-1781225e1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy import displacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6023a4-4863-48c0-966e-aa0abed61905",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"span_ruler\")\n",
    "text= \"Hi welcome to AIRE500 , this is captain pk.PRITHVI KIRAN ON ROLL CALL #334#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b117f966-5db1-4ca1-8a2f-9a8dfc86ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def AirlineCodes(text):\n",
    "\n",
    "    patterns = [\n",
    "    {\"label\": \"AirlineCode\", \"pattern\": r\"^[A-Z]{4}\\d{3}$\"},\n",
    "    {\"label\":\"ID\",\"pattern\":\"#123#\"},\n",
    "    {\"label\":\"ID\",\"pattern\":\"#342#\"},\n",
    "    {\"label\":\"ID\",\"pattern\":\"#456#\"},\n",
    "    {\"label\":\"ID\",\"pattern\":\"#135#\"},\n",
    "    {\"label\":\"SEC\",\"pattern\":\"IPL\"},\n",
    "    {\"label\": \"INDIA\", \"pattern\": [{\"ORTH\": \"+91\"}, {\"SHAPE\": \"dddddddddd\"}]},\n",
    "    {\"label\": \"USA or CANADA\", \"pattern\": [{\"ORTH\": \"+1\"}, {\"SHAPE\": \"ddddddddd\"}]}\n",
    "    ]\n",
    "    ruler.add_patterns(patterns)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        print (ent.text, ent.label_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7d0e35-3569-40b0-a1f4-0580ffbfe58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#334# MONEY\n"
     ]
    }
   ],
   "source": [
    "AirlineCodes(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8011576-274f-4359-97d5-5dcdf22b6103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15578876784678163569 HelloWorld 0 3 Hello, world\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def find\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = doc[start:end]\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a29cd0-7e36-49c4-9e9a-e7e625529326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG'), ('San Francisco', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ruler = nlp.add_pipe(\"span_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(span.text, span.label_) for span in doc.spans[\"ruler\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea8dfb24-1c65-4aaf-a54f-a41c9abfdf54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Simstring_Spell_Correction_English_Words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSimstring_Spell_Correction_English_Words\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Simstring_Spell_Correction_Check_English_Words\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filter_spans\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define custom NER labels and keywords\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Simstring_Spell_Correction_English_Words'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.language import Language\n",
    "import pandas as pd\n",
    "from Simstring_Spell_Correction_English_Words import Simstring_Spell_Correction_Check_English_Words\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "# Define custom NER labels and keywords\n",
    "CUSTOM_NER_PURCHASE_ORDER = \"PURCHASE ORDER NUMBER\"\n",
    "PURCHASE_ORDER_KEYWORDS = [\"PO Number:\", \"PO #\", \"PO#\", \"P.O. Number\", \"PO Number\", \"PURCHASE ORDER NUMBER\",\n",
    "                           \"PURCHASEORDER NUMBER\", \"PURCHASE ORDERNUMER\", \"PURCHASE ORDER NO.\", \"PURCHASEORDER NO.\",\n",
    "                           \"PURCHASE ORDERNO.\", \"STANDARD PURCHASE ORDER\", \"PURCHASE ORDER #\", \"P.O. No.\", \"PO#\",\n",
    "                           \"PURCHASE ORDER NUMBER\", \"P.O.No.\", \"P.O.\"]\n",
    "\n",
    "CUSTOMER_NER_TRANS_NUMBER = \"TRANS NUMBER\"\n",
    "TRANS_NUMBER_KEYWORDS = [\"Trans Number:\", \"Trans Number :\", \"TransNumber:\", \"TransNumber :\", \"Trans#\", \"Trans #\"]\n",
    "\n",
    "CUSTOM_NER_LABEL_DATE = \"INVOICE DATE\"\n",
    "INVOICE_DATE_KEYWORDS = [\"InvoiceDate\", \"Invoice Date\", \"Date Check Required\", \"DateCheckRequired\",\n",
    "                         \"DateCheck Required\", \"Date CheckRequired\", \"Date Requested\", \"DateRequested\"]\n",
    "\n",
    "CUSTOM_NER_LABEL_NUMBER = \"INVOICE NUMBER\"\n",
    "INVOICE_NUMBER_KEYWORDS = [\"InvoiceNumber\", \"Invoice No\", \"Invoice No.\", \"Invoice#\", \"Invoice\",\n",
    "                           \"Invoice #\", \"INV#\", \"INV Number\", \"INV Number\", \"Invoice Number\",\n",
    "                           \"Invoice:\", \"Invoice :\", \"Invoice-\", [\"Invoice\", \"#\", \":\"], [\"Invoice\", \"#\", \":\", \" \"],\n",
    "                           ['Invoice', 'Number', ':'], ['Invoice', 'Number', ':', \" \"], [\"Invoice\", \":\"],\n",
    "                           [\"Invoice\", \" \", \":\"], [\"Invoice\", \":\", \" \"], [\"Invoice:\"], [\"Inv\", \"#\"],\n",
    "                           [\"Invoice\", \"ID\", \":\"], [\"-\", \"Invoice\", \"-\"], [\"-\", \"Invoice\"]]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "def create_patterns(keywords):\n",
    "    patterns = []\n",
    "    for keyword in keywords:\n",
    "        # Check if the keyword is a string or a list\n",
    "        if isinstance(keyword, str):\n",
    "            tokens = keyword.split()\n",
    "        else:\n",
    "            tokens = keyword\n",
    "\n",
    "        # Create a pattern for each token\n",
    "        pattern = [{\"LOWER\": token.lower()} for token in tokens]\n",
    "        patterns.append(pattern)\n",
    "\n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Add patterns to matcher\n",
    "invoice_date_patterns = create_patterns(INVOICE_DATE_KEYWORDS)\n",
    "matcher.add(CUSTOM_NER_LABEL_DATE, invoice_date_patterns)\n",
    "\n",
    "invoice_number_patterns = create_patterns(INVOICE_NUMBER_KEYWORDS)\n",
    "matcher.add(CUSTOM_NER_LABEL_NUMBER, invoice_number_patterns)\n",
    "\n",
    "purchase_order_pattern = create_patterns(PURCHASE_ORDER_KEYWORDS)\n",
    "matcher.add(CUSTOM_NER_PURCHASE_ORDER, purchase_order_pattern)\n",
    "\n",
    "trans_number_pattern = create_patterns(TRANS_NUMBER_KEYWORDS)\n",
    "matcher.add(CUSTOMER_NER_TRANS_NUMBER, trans_number_pattern)\n",
    "\n",
    "pattern_invoice_hashtag_combined = [{\"LOWER\": {\"REGEX\": \"invoice#\"}}]\n",
    "matcher.add(CUSTOM_NER_LABEL_NUMBER, [pattern_invoice_hashtag_combined])\n",
    "\n",
    "# Define pattern for \"invoice\" followed by \"#\"\n",
    "pattern_invoice_followed_by_hashtag = [{\"LOWER\": \"invoice\"}, {\"TEXT\": \"#\"}]\n",
    "matcher.add(CUSTOM_NER_LABEL_NUMBER, [pattern_invoice_followed_by_hashtag])\n",
    "\n",
    "\n",
    "def correct_spelling(input_text, similarity_score=0.6):\n",
    "    input_words = [word for word in input_text.split()]\n",
    "    simstring_output = Simstring_Spell_Correction_Check_English_Words(input_words, similarity_score)\n",
    "    correction_map = {}\n",
    "    for correction in simstring_output:\n",
    "        try:\n",
    "            original_word = correction[0]['Input_Word']\n",
    "            corrected_word = correction[0]['Get_word']\n",
    "            correction_map[original_word] = corrected_word\n",
    "        except (IndexError, KeyError):\n",
    "            pass\n",
    "    corrected_input_words = [correction_map.get(word, word) for word in input_words]\n",
    "    corrected_input_text = \" \".join(corrected_input_words)\n",
    "    return corrected_input_text\n",
    "\n",
    "\n",
    "@Language.component(\"custom_ner_component\")\n",
    "def custom_ner_component(doc):\n",
    "    matches = matcher(doc)\n",
    "    new_ents = []\n",
    "    existing_entities = [(ent.start, ent.end) for ent in doc.ents]\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        if span:\n",
    "            overlap = any(start >= ent_start and end <= ent_end for ent_start, ent_end in existing_entities)\n",
    "            if overlap:\n",
    "                conflicting_ents = [ent for ent in doc.ents if not (ent.start < end and ent.end > start)]\n",
    "                doc.ents = [ent for ent in doc.ents if not (ent.start >= start and ent.end <= end)]\n",
    "                existing_entities = [(ent.start, ent.end) for ent in doc.ents]\n",
    "            new_ent = spacy.tokens.Span(doc, start, end, label=nlp.vocab.strings[match_id])\n",
    "            new_ents.append(new_ent)\n",
    "            existing_entities.append((start, end))\n",
    "\n",
    "    # Assuming `new_ents` is your list of new entities\n",
    "    new_ents = filter_spans(new_ents)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"custom_ner_component\", after=\"ner\")\n",
    "\n",
    "\n",
    "def NER_regex(object_df, batch_size=5):\n",
    "    listall = []\n",
    "    lines = object_df['LineNum'].unique()\n",
    "    line_batches = [lines[i:i + batch_size] for i in range(0, len(lines), batch_size)]\n",
    "    batch_count = 0\n",
    "    for batch_lines in line_batches:\n",
    "        batch_count += 1\n",
    "        batch_text = []\n",
    "        batch_lengths = []  # Reset batch lengths for each batch\n",
    "        for line_num in batch_lines:\n",
    "            objects_on_line = object_df.loc[object_df['LineNum'] == line_num, 'Object'].tolist()\n",
    "            batch_length = [len(obj) for obj in objects_on_line]\n",
    "            batch_lengths.append(batch_length)\n",
    "            batch_text.append(\" \".join(objects_on_line))\n",
    "\n",
    "        batch_text = \"\\n\".join(batch_text)\n",
    "        corrected_batch_text = correct_spelling(batch_text.strip())\n",
    "        doc = nlp(corrected_batch_text)\n",
    "        ner_entities = [{\"text\": ent.text, \"label\": ent.label_, \"start\": ent.start_char, \"end\": ent.end_char} for ent in\n",
    "                        doc.ents]\n",
    "\n",
    "        # Check if there are any NER entities detected\n",
    "        if ner_entities:\n",
    "            result = {\"Object\": batch_text, \"NER\": ner_entities, 'Batch_lengths': batch_lengths, 'Batch': batch_count}\n",
    "        else:\n",
    "            result = {\"Object\": batch_text, \"NER\": [], 'Batch_lengths': batch_lengths, 'Batch': batch_count}\n",
    "\n",
    "        listall.append(result)\n",
    "\n",
    "    df = pd.DataFrame(listall)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.DataFrame({'LineNum': [1, 1, 2, 2], 'Object': ['PO Number: 123', 'Date: 2023-05-01', 'Invoice Number: 456', 'Amount: $789']})\n",
    "# result_df = NER_regex(df)\n",
    "# print(result_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60243a-9fc8-492e-8bdd-8f2bf19eec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
